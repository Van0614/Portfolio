
# -*- coding: utf-8 -*-
"""DMA_project2_team01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lkSLVZzum_SNeBCiAGENjYO7UNS4bucP
"""

# TODO: CHANGE THIS FILE NAME TO DMA_project2_team##.py
# EX. TEAM 1 --> DMA_project2_team01.py

# TODO: IMPORT LIBRARIES NEEDED FOR PROJECT 2
import mysql.connector
import os
import surprise
from surprise import Dataset
from surprise import Reader
from surprise.model_selection import KFold
from collections import defaultdict
import numpy as np
import pandas as pd
from sklearn import tree
import graphviz
from mlxtend.frequent_patterns import association_rules, apriori

np.random.seed(0)

# TODO: CHANGE GRAPHVIZ DIRECTORY
# If you installed graphviz with the command conda install python-graphviz at the anaconda prompt, you would not need the following procedure.
# os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz2.47.1/bin/'
# os.environ["PATH"] += os.pathsep + '/usr/local/Cellar/graphviz/2.47.1/bin/' # for MacOS

# TODO: CHANGE MYSQL INFORMATION, team number 
HOST = 'localhost'
USER = 'root'
PASSWORD = 'yuchang0712@@'
SCHEMA = 'DMA_team01'
team = 1


# PART 1: Decision tree 
def part1():
    cnx = mysql.connector.connect(host=HOST, user=USER, password=PASSWORD)
    cursor = cnx.cursor()
    cursor.execute('SET GLOBAL innodb_buffer_pool_size=2*1024*1024*1024;')
    cursor.execute('USE %s;' % SCHEMA)

    # TODO: Requirement 1-1. MAKE best_item column
    #cursor.execute('ALTER TABLE anime DROP COLUMN best_anime;') 
    cursor.execute('ALTER TABLE DMA_team01.anime ADD COLUMN best_anime TINYINT(1) default 0;')

    fopen = open("best_anime_list.txt", 'r')
    lines = fopen.readlines()  # txt를 한 줄씩 읽어올 예정
    for line in lines:
        line = line.strip()  # 각 줄 마지막의 "줄 바꿈 문자"를 제거한다.
        cursor.execute('UPDATE DMA_team01.anime SET best_anime = 1 WHERE id = (%s)',(line,))  # best_anime에 해당하는 id에 대해 1로 값을 바꾼다.
    fopen.close()

    # TODO: Requirement 1-2. WRITE MYSQL QUERY AND EXECUTE. SAVE to .csv file

    cursor.execute('''
    SELECT a.id AS id, a.best_anime AS best_anime, R2.avg AS ratings, COUNT(ag.genre_id) AS num_of_genres, part.cou AS num_of_total_users, part.cou1 AS num_of_users_by_ws1, part.cou2 AS num_of_users_by_ws2, part.cou3 AS num_of_users_by_ws3, part.cou4 AS num_of_users_by_ws4, part.cou5 AS num_of_users_by_ws5, part.cou6 AS num_of_users_by_ws6, a.Source AS source    
    FROM anime AS a 
    INNER JOIN anime_genre AS ag ON a.id = ag.anime_id 
    INNER JOIN
    (
        SELECT R.anime_id AS a_id, AVG(R.rating) AS avg
        FROM anime_user_rating AS R
        GROUP BY R.anime_id
    ) R2 ON a.id = R2.a_id
    INNER JOIN 
    (
        SELECT aus.anime_id AS a_id,
            COUNT(*) AS cou,
            SUM(CASE WHEN aus.watching_status = 1 THEN 1 ELSE 0 END) AS cou1,
            SUM(CASE WHEN aus.watching_status = 2 THEN 1 ELSE 0 END) AS cou2,
            SUM(CASE WHEN aus.watching_status = 3 THEN 1 ELSE 0 END) AS cou3,
            SUM(CASE WHEN aus.watching_status = 4 THEN 1 ELSE 0 END) AS cou4,
            SUM(CASE WHEN aus.watching_status = 5 THEN 1 ELSE 0 END) AS cou5,
            SUM(CASE WHEN aus.watching_status = 6 THEN 1 ELSE 0 END) AS cou6
        FROM anime_user_status AS aus    
        GROUP BY aus.anime_id
    ) part ON a.id = part.a_id
    GROUP BY a.id;
    ''')

    fopen = open('DMA_project2_team%02d_part1.csv' % team, 'w', encoding='utf-8')

    column_name = ('id', 'best_anime', 'ratings', 'num_of_genres', 'num_of_totatl_users','num_of_users_by_ws1', 
                  'num_of_users_by_ws2', 'num_of_users_by_ws3', 'num_of_users_by_ws4', 'num_of_users_by_ws5', 'num_of_users_by_ws6', 'source') # PART 01.CSV에 COLUME NAME 추가

    fopen.write(','.join(column_name) + '\n')

    for row in cursor:
        all_strings = tuple(map(str, row))
        fopen.write(','.join(all_strings) + '\n')

    fopen.close()

    # TODO: Requirement 1-3. MAKE AND SAVE DECISION TREE
    # gini file name: DMA_project2_team##_part1_gini.pdf
    # entropy file name: DMA_project2_team##_part1_entropy.pdf

    # test code
    # with open('DMA_project2_team01_part1.csv', 'a') as f:
    #     f.write('2000,1,,5,887,43,324,60,55,0,405,Manga')
    #     f.write("\n")

    df = pd.read_csv('DMA_project2_team%02d_part1.csv' % team)

    # X(features)와 Y(classes) 분리, 여기에서 Y는 best_anime에 해당
    features = df.drop(["best_anime"], axis=1)
    classes = df["best_anime"]

    # ratings column의 결측치를 5로 대체
    features["ratings"].fillna(5, inplace=True)

    # categorical data에 해당하는 genre를 one hot encoding
    features = pd.get_dummies(features, columns=['source'])

    # # Decision Tree 통한 분석 단계
    # # 1. gini
    dt_args = {
        "criterion": "gini",
        "min_samples_leaf": 10,
        "max_depth": 5,
        "random_state": 0
    }
    DT = tree.DecisionTreeClassifier(**dt_args)
    DT.fit(X=features, y=classes)
    # 아래 주석처리한 부분은 보고서 작성을 위한 것
    # print(features.columns.to_list())
    # print(DT.feature_importances_)
    # for feature, importance in zip(features.columns.to_list(), DT.feature_importances_):
    #     print(f"{feature}: {importance}")
    graph = tree.export_graphviz(DT, out_file=None,
                                 feature_names=features.columns.to_list(),
                                 class_names=['normal', 'BEST'])
    graph = graphviz.Source(graph)
    graph.render('DMA_project2_team%02d_part1_gini' % team, view=True)
    #
    # 2. entropy
    dt_args["criterion"] = "entropy"
    DT = tree.DecisionTreeClassifier(**dt_args)
    DT.fit(X=features, y=classes)
    # 아래 주석처리한 부분은 보고서 작성을 위한 것
    # for feature, importance in zip(features.columns.to_list(), DT.feature_importances_):
    #     print(f"{feature}: {importance}")
    graph = tree.export_graphviz(DT, out_file=None,
                                 feature_names=features.columns.to_list(),
                                 class_names=['normal', 'BEST'])
    graph = graphviz.Source(graph)
    graph.render('DMA_project2_team%02d_part1_entropy' % team, view=True)

    # TODO: Requirement 1-4. Don't need to append code for 1-4

    cursor.close()
    

# PART 2: Association analysis
def part2():
    cnx = mysql.connector.connect(host=HOST, user=USER, password=PASSWORD)
    cursor = cnx.cursor()
    cursor.execute('SET GLOBAL innodb_buffer_pool_size=2*1024*1024*1024;')
    cursor.execute('USE %s;' % SCHEMA)

    # TODO: Requirement 2-1. CREATE VIEW AND SAVE to .csv file

    fopen = open('DMA_project2_team%02d_part2_anime.csv' % team, 'w', encoding='utf-8')
    
    # VIEW 생성
    cursor.execute('''
    CREATE OR REPLACE VIEW anime_score AS 
    SELECT id AS anime_id, Name as anime_name, num_genre, num_user, num_genre_anime, num_genre+num_user+num_genre_anime AS score
    FROM anime AS a
    NATURAL JOIN 
    (
    SELECT id, 100*COUNT(genre_id) AS num_genre
    FROM anime
    INNER JOIN anime_genre ON id=anime_id
    GROUP BY id
    ) ag2
    NATURAL JOIN 
    (
    SELECT id, COUNT(user_id) AS num_user
    FROM anime
    INNER JOIN anime_user_status ON id=anime_id
    GROUP BY id
    ) aus2
    NATURAL JOIN
    (
    SELECT id, AVG(cnt) AS num_genre_anime
    FROM anime
    INNER JOIN anime_genre ON id=anime_id
    NATURAL JOIN
    (
    SELECT genre_id, COUNT(anime_id) AS cnt
    FROM anime_genre
    GROUP BY genre_id
    ) ag3
    GROUP BY id
    ) aus3
    ORDER BY score DESC
    LIMIT 100;
    ''')


    # csv로 저장
    cursor.execute('''
    SELECT *
    FROM anime_score;
    ''')

    column_name = ('anime_id', 'anime_name', 'num_genre', 'num_user', 'num_genre_anime','score') # PART 02.CSV에 COLUME NAME 추가

    fopen.write(','.join(column_name) + '\n')

    for row in cursor:
        all_strings=tuple(map(str,row))
        fopen.write(','.join(all_strings)+'\n')

    fopen.close()


    # TODO: Requirement 2-2. CREATE 2 VIEWS AND SAVE partial one to .csv file

    fopen = open('DMA_project2_team%02d_part2_UAI.csv' % team, 'w', encoding='utf-8')

    # VIEW: user_anime_IntDegree 생성
    cursor.execute('''
    CREATE OR REPLACE VIEW user_anime_IntDegree AS
    SELECT user_id AS user, anime_name AS anime, AVG(cnt)+LEAST(COUNT(genre_id),5) AS IntDegree       
    FROM anime_user_status AS aus
    NATURAL JOIN anime_score 
    NATURAL JOIN anime_genre
    NATURAL JOIN
    (
    SELECT user_id, genre_id, COUNT(anime_id) AS cnt
    FROM
    (
    SELECT *
    FROM anime_user_status
    ) aus2
    NATURAL JOIN anime_score
    NATURAL JOIN anime_genre
    GROUP BY user_id, genre_id
    ) ugc
    GROUP BY user_id, anime_id;
    ''')

    # VIEW: partial_user_anime_IntDegree 생성
    cursor.execute('''
    CREATE OR REPLACE VIEW partial_user_anime_IntDegree AS
    SELECT *
    FROM user_anime_IntDegree AS uai
    WHERE uai.user IN
    (
    SELECT user
    FROM user_anime_IntDegree
    GROUP BY user
    HAVING COUNT(anime)>19
    );
    ''')

    #csv 생성
    cursor.execute('''
    SELECT *
    FROM partial_user_anime_IntDegree;
    ''')

    column_name = ('user', 'anime', 'IntDegree') # PART 02.CSV에 COLUME NAME 추가

    fopen.write(','.join(column_name) + '\n')

    for row in cursor:
        all_strings = tuple(map(str, row))
        fopen.write(','.join(all_strings) + '\n')

    fopen.close()


    # TODO: Requirement 2-3. MAKE HORIZONTAL VIEW
    # file name: DMA_project2_team##_part2_horizontal.pkl
    # use to_pickle(): df.to_pickle(filename)

    cursor.execute('''
    SELECT user, anime
    FROM partial_user_anime_IntDegree
    ''')

    view = pd.DataFrame(cursor.fetchall())
    view.columns = cursor.column_names
    horizontal_view=pd.pivot_table(view, index=['user'], columns=['anime'], fill_value=0, aggfunc=len)
    horizontal_view.to_pickle("./DMA_project2_team01_part2_horizontal.pkl")

    # TODO: Requirement 2-4. ASSOCIATION ANALYSIS
    # filename: DMA_project2_team##_part2_association.pkl (pandas dataframe)

    frequent_itemsets=apriori(horizontal_view,min_support=0.35, use_colnames=True)
    rules=association_rules(frequent_itemsets, metric='lift', min_threshold=1.4)
    rules.to_pickle("./DMA_project2_team01_part2_association.pkl")

    cursor.close()

# TODO: Requirement 3-1. WRITE get_top_n
def get_top_n(algo, testset, id_list, n, user_based=True):
    results = defaultdict(list)
    if user_based:
        # TODO: testset의 데이터 중에 user id가 id_list 안에 있는 데이터만 따로 testset_id로 저장
        # Hint: testset은 (user_id, anime_name, default_rating)의 tuple을 요소로 갖는 list

        testset_id = []
        for datas in testset:
            if datas[0] in id_list:
                testset_id.append(datas)

        predictions = algo.test(testset_id)

        for uid, bname, true_r, est, _ in predictions:
            # TODO: results는 user_id를 key로, [(anime_name, estimated_rating)의 tuple이 모인 list]를 value로 갖는 dictionary

            results[uid].append((bname, est))



    else:
        # TODO: testset의 데이터 중 anime name이 id_list 안에 있는 데이터만 따로 testset_id라는 list로 저장
        # Hint: testset은 (user_id, anime_name, default_rating)의 tuple을 요소로 갖는 list
        testset_id = []
        for datas in testset:
            if datas[1] in id_list:
                testset_id.append(datas)

        predictions = algo.test(testset_id)
        for uid, bname, true_r, est, _ in predictions:
            # TODO: results는 anime_name를 key로, [(user_id, estimated_rating)의 tuple이 모인 list]를 value로 갖는 dictionary

            results[bname].append((uid, est))

    for id_, ratings in results.items():
        # TODO: rating 순서대로 정렬하고 top-n개만 유지

        sorted_ratings = sorted(ratings, key=lambda x: x[1], reverse=True)[0:n]
        results[id_] = sorted_ratings

    return results


# PART 3. Requirement 3-2, 3-3, 3-4
def part3():
    file_path = 'DMA_project2_team%02d_part2_UAI.csv' % team
    reader = Reader(line_format='user item rating', sep=',', rating_scale=(1, 30), skip_lines=1)
    data = Dataset.load_from_file(file_path, reader=reader)

    trainset = data.build_full_trainset()
    testset = trainset.build_anti_testset()
    # TODO: Requirement 3-2. User-based Recommendation
    uid_list = ['1496', '2061', '2324', '4041', '4706']

    # TODO: set algorithm for 3-2-1
    sim_options={'name':'cosine', 'user_based':True}
    algo = surprise.KNNBasic(sim_options=sim_options)

    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)

    with open('3-2-1.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: set algorithm for 3-2-2
    sim_options = {'name': 'pearson', 'user_based': True}
    algo = surprise.KNNWithMeans(sim_options=sim_options)

    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)
    with open('3-2-2.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: 3-2-3. Best Model
    best_algo_ub = None
    best_acc=0 #best accuracy
    sim_options={}
    best_sim_option={}
    algo_list=[surprise.KNNWithZScore, surprise.KNNBasic, surprise.KNNBaseline, surprise.KNNWithMeans]
    sim_list=['cosine', 'msd', 'pearson', 'pearson_baseline']
    sim_options_list=[]

    #creating sim_options_list
    for sim in sim_list:
        sim_options['name']=sim
        sim_options['user_based']=True
        sim_options_list.append(sim_options)
    
    #get the best model
    for alg in algo_list:
        for sim_option in sim_options_list:
            algo=alg(sim_options=sim_option)
            kf = KFold(n_splits=5, random_state=0)
            acc = []
            for i, (train_set, test_set) in enumerate(kf.split(data)):
                algo.fit(train_set)
                predictions = algo.test(test_set)
                acc.append(surprise.accuracy.rmse(predictions, verbose=False))
            if np.mean(acc)>best_acc:
                best_algo_ub=algo
                best_acc = np.mean(acc)
                best_sim_option = sim_option
    print("Best user-based model:%s" % best_algo_ub)
    print("Average accuracy: %f" % best_acc) #RMSE
    print("sim_option:%s" % best_sim_option)


    # TODO: Requirement 3-3. Item-based Recommendation
    bname_list = ['Azumanga Daioh',
                  'InuYasha',
                  'Death Note',
                  'Shijou Saikyou no Deshi Kenichi',
                  'Fruits Basket']
    # TODO - set algorithm for 3-3-1
    sim_options = {'name': 'cosine', 'user_based': False}
    algo = surprise.KNNBasic(sim_options=sim_options)

    algo.fit(trainset)
    results = get_top_n(algo, testset, bname_list, n=10, user_based=False)
    with open('3-3-1.txt', 'w') as f:
        for bname, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('Anime NAME %s top-10 results\n' % bname)
            for uid, score in ratings:
                f.write('User ID %s\n\tscore %s\n' % (uid, str(score)))
            f.write('\n')

    # TODO: set algorithm for 3-3-2
    sim_options = {'name': 'pearson', 'user_based': False}
    algo = surprise.KNNWithMeans(sim_options=sim_options)

    algo.fit(trainset)
    results = get_top_n(algo, testset, bname_list, n=10, user_based=False)
    with open('3-3-2.txt', 'w') as f:
        for bname, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('Anime NAME %s top-10 results\n' % bname)
            for uid, score in ratings:
                f.write('User ID %s\n\tscore %s\n' % (uid, str(score)))
            f.write('\n')

    # TODO: 3-3-3. Best Model
    best_algo_ib = None
    best_acc = 0  # best accuracy
    sim_options = {}
    best_sim_option={}
    algo_list = [surprise.KNNWithZScore, surprise.KNNBasic, surprise.KNNBaseline, surprise.KNNWithMeans]
    sim_list = ['cosine', 'msd', 'pearson', 'pearson_baseline']
    sim_options_list = []

    # creating sim_options_list
    for sim in sim_list:
        sim_options['name'] = sim
        sim_options['user_based'] = False
        sim_options_list.append(sim_options)

    # get the best model
    for alg in algo_list:
        for sim_option in sim_options_list:
            algo = alg(sim_options=sim_option)
            kf = KFold(n_splits=5, random_state=0)
            acc = []
            for i, (train_set, test_set) in enumerate(kf.split(data)):
                algo.fit(train_set)
                predictions = algo.test(test_set)
                acc.append(surprise.accuracy.rmse(predictions, verbose=False))
            if np.mean(acc) > best_acc:
                best_acc=np.mean(acc)
                best_algo_ib = algo
                best_sim_option= sim_option
    print("Best item-based model:%s"%best_algo_ib)
    print("Average accuracy: %f"%best_acc) #RMSE
    print("sim_option:%s"%best_sim_option)

    # TODO: Requirement 3-4. Matrix-factorization Recommendation
    # TODO: set algorithm for 3-4-1
    algo = surprise.SVD(n_factors=100, n_epochs=50, biased=False)

    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)
    with open('3-4-1.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: set algorithm for 3-4-2
    algo = surprise.SVD(n_factors=200, n_epochs=100, biased=True)  

    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)
    with open('3-4-2.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: set algorithm for 3-4-3
    algo = surprise.SVDpp(n_factors=100, n_epochs=50)

    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)
    with open('3-4-3.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: set algorithm for 3-4-4
    algo = surprise.SVDpp(n_factors=100, n_epochs=100)
    
    algo.fit(trainset)
    results = get_top_n(algo, testset, uid_list, n=5, user_based=True)
    with open('3-4-4.txt', 'w') as f:
        for uid, ratings in sorted(results.items(), key=lambda x: x[0]):
            f.write('User ID %s top-5 results\n' % uid)
            for bname, score in ratings:
                f.write('Anime NAME %s\n\tscore %s\n' % (bname, str(score)))
            f.write('\n')

    # TODO: 3-4-5. Best Model
    best_algo_mf = None
    best_acc = 0  # best accuracy
    sim_options = {}
    algo_list = [surprise.SVD, surprise.SVDpp, surprise.NMF]
    n_factors_list = [20, 40, 60, 80, 100]
    n_epochs_list = [50, 100]
    
    # get the best model
    for alg in algo_list:
        for n_factors in n_factors_list:
            for n_epochs in n_epochs_list:
                algo = alg(n_factors=n_factors, n_epochs=n_epochs)
                kf = KFold(n_splits=5, random_state=0)
                acc = []
                for i, (train_set, test_set) in enumerate(kf.split(data)):
                    algo.fit(train_set)
                    predictions = algo.test(test_set)
                    acc.append(surprise.accuracy.rmse(predictions, verbose=False))
                    if (np.mean(acc) > best_acc):
                        best_acc = np.mean(acc)
                        best_algo_mf = algo
                        best_algo_mf_n_factors = n_factors
                        best_algo_mf_n_epochs = n_epochs

    print("Best matrix-based model:%s" % best_algo_mf)
    print("Average accuracy: %f" % best_acc) #RMSE
    print("n_factors:%s" % best_algo_mf_n_factors)
    print("n_epochs:%s" % best_algo_mf_n_epochs)
    
       

if __name__ == '__main__':
    part1()
    part2()
    part3()
